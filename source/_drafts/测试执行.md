---
title: 测试杂谈
date: 2018-01-02 14:48:54
tags:
- test
---

## 测试日常

测试这个行业经常会被吐槽到"不就是点点点嘛"，这时候我一般都会补充到"嗯，有计划有规律的点点点"。毕竟selenium自动化脚本也得模拟点击，搞性能测试-模拟用户点点点，搞安全测试-模拟黑客点点点。感受不到技术的深度，做的好坏不能引起重视。但这个看似可有可无的职业却一直在发展，人才市场的需求也在不断地增长，为什么？是我们的项目需要一个背锅人？不，是因为使用者对我们的产品有要求。而我们测试促进了产品最大程度的接近这个要求。So，与其自怨自艾英雄无用武之地，不如花更多时间去思考如何把工作做好，即使是深受吐槽的点点点。

以下是引用自知乎的一段话，很明确地阶段化了我们的测试工作

> 从测试中遇到问题采取的行动来看，我观察到的测试人员能达到的层次大概有这么几个级别：
>
> 1. 开一个bug
> 2. 查找一些额外的资料如设计文档和历史，确定这是一个问题，然后给出详细的bug重现步骤
> 3. 对重现步骤做一些精炼，确定能够重现bug的最少步骤；可能的话，将重现步骤做自动化
> 4. 尝试通过研究代码确认问题所在
> 5. 尝试给出一个fix
> 6. 对错误的原因进行分析，提出一些标准化的方法来检测出类似的问题，比如stress，fuzzing等等
> 7. 能够对标准化的测试流程定义对应的数据分析方法，可以保证开发和项目主管都能从中得到需要的信息来掌控质量状况。

大多数情况下我们都停留在2阶段并对3阶段展望，而且不得不承认当项目紧张起来的时候，测试空间被压缩，于是就只简单的"开一个bug"。如果作为开发，忙活了大段时间，结果迎来的是杂乱不堪的各种bug，谁的心情又会好呢？这时候通常就会进入到甩锅环节，可对项目却没一点帮助。那么我们测试该怎么做，才算是有帮助呢？我整理了一下个人经历和看过的一些测试书籍提炼了一些可能有帮助的点，以供参考。

## 如何规划测试时间

一般项目开始的时候，我们会有一个计划来估计测试时间，用来规范项目开始后的测试进度。通常都是参考开发的时间来制定，但仍然有其它因素需要考虑进去，例如借鉴之前完成的项目、衡量软件的复杂度、了解需求所解决的问题及其要达到的程度。

软件开发的不确定性往往会打乱我们的项目进度。这种不确定性可能来源于难以深度解读用户的需求，技术人员所掌握的技能有限，甚至是市场的变幻等等。一旦发生这种不确定，改设计、改需求都是必然的。而我们的测试时间也不会得到预留，时间被压缩，匆匆在deadline之前完成任务。要保证测试完成度，一定要有一个测试时间最小值。

就单拿执行用例来举个例子，可以通过用例步骤和该用例所需要测试的环境来做工作量的预估。假设用例每一步平均用时20s，一个用例平均6步左右，那么执行一个用例就120s(2min)。若所测版本有600个用例，在不被任何情况打断的条件下得花1200min也就是20小时。而一天的工作中完全在执行用例的时间顶多5小时，所以可能得4天才能执行完这600个用例，还不能有阻碍测试的情况。

想要规划好时间，真得团队负责人有丰富的经验，还要了解手下的那一批人。大改需求还要会约法三章，要么硬着头皮上，要么不上再加个版本的时间来改。强行改出来的产品只会更不尽如人意。

## 需求不明确怎么办？

需求不明甚至没有，是现今互联网公司相当常见的一个问题。不说巧妇难为无米之炊，因为你必须，所以你可以。这里推荐两个解决方法：

1. **问**。直接找相关负责人问，有产品问产品，没产品问开发。问一下这个软件应该如何工作。问一下这个软件如何处理数据。问一下这个软件如何处理错误。
2. **看**。直接看源代码，运行应用程序，毕竟真到了得问开发的程度了，那就八仙过海各显神通吧。通过打断点、调试和`Ctrl+Shift+F10` ，让程序跑起来，自己看应用是如何运作的。


通常来说，多数测试更了解业务功能，问起来就相对容易，但不定有可以解答的人啊。能看懂代码一定会是一个硬性要求。看不懂就找开发负责人，早早规划好测试人员的培训。

## 怎样保证测试工作的进行

一个好的测试策略为团队提供了愿景，帮助每个人决定什么测试活动是最重要的以及如何去应用不同种类的测试。 这个策略包括测试的种类、进程和测试时测试团队会采用什么方法。它包括对风险的预估以帮助团队决定错误最有可能发生在哪里或者某些组件是否可能需要更加全面彻底的测试。

测试策略确定了测试活动中的什么是最重要的，明确了测试团队对应不同种类的测试应该采用的方法。而且，策略也包括了项目的风险评估，预测功能中最有可能发生错误的地方，和该对哪些组件进行更加全面彻底的测试。测试策略通常也包括了对测试团队的培训，已满足测试进行中执行测试所要求掌握的知识和技能。

## 测试用例粒度的掌握

测试用例的测试过程通常可以一步完成，有时需要连续的几步。通常会给出期望的结果或现象。除此之外，还可以给出如下信息：

- 测试用例ID
- 测试用例描述
- 测试步骤或执行次序
- 相关依赖
- 测试类别
- 负责人
- 是否为自动化测试

测试完成后还可以追加或完善如下信息：

- 是否通过
- 备注

大型测试用例还可能包括前提状态及相关描述。书面格式的测试用例还应含有填写实际测试结果的空间。

创建好的测试用例是一个困难的过程。即使一个错误就可以毁掉测试用例的意图。一些易出问题的领域如下：

* 步骤缺乏 会造成不能准确地重现。 
* 太多细节 虽然提供具体的信息很重要，但是不必要的字词或冗长的解释，会使测试用例难以遵循。
* 行话太多 不要以为运行测试用例的人都知道所有你写的缩略语、代号和缩写。
* 不明确的检验标准 如果执行用例后，不清楚测试是否通过或失败，那测试用例是毫无用处的。


## 缺陷不该被用来做绩效度量

通过分析缺陷数据可以掌握的

* 所测试功能的复杂性
* 开发人员编程能力
* 规格完整性
* 缺陷预防与缺陷发现
* 报告的及时性

若使用缺陷作为绩效度量需要解决的问题：

* 缺陷报告中问题的严重程度和优先级是如何分布的？
* 功能缺陷与简单的用户界面缺陷算一样的数量吗？
* 花时间追踪一个关键问题（如数据丢失，内存泄漏）并使之得到解决，这能说明没有达到预期或业绩表现差吗？如果是，那为什么要有协助开发人员排解疑难问题的团队合作的政策？
* 如何划分缺陷等级？不同缺陷等级之间又如何评比？
* 每一次评比，最少得发现多少缺陷？而发现多少缺陷又算是超常发挥了呢？

发现了大量的缺陷可能表明测试人员做的很好，或者它可能意味着开发人员编写的代码很差。反之，如果一个测试人员找到很少的缺陷，这可能是一个迹象:表明他做得不理想，也可能意味着他正在测试具有较低的缺陷密度高质量的代码。所以关键是怎样解读数据，这也意味着可能需要额外的个案调查。

例如，如果一个测试人员没有报告很多缺陷，看一下功能区以确定是什么原因造成缺陷数量低。如果其他用户（客户、开发，Beta测试用户）在该功能区找到缺陷，该测试人员的低缺陷数可能会有问题。当然，如果进一步的调查后，您确定功能区的测试不错，没有多少缺陷，这当然就不该怪测试人员了。

从我工作的角度上来看，真要有标准来考量测试的话，那么从产品、开发和用户的反馈来考量是比较得当的。

注：《微软的软件测试之道》看过之后，明显发现书中提及到的很多知识，现在更加的规范和完整了，而且很多额外的步骤也被精简了。